Day6 How to Fine-Tune BERT for Text Classification?

### BERT-base Model
- Consists of an encoder with 12 Transformer blocks, 12 self-attention heads, and a hidden layer size of 768
- Accepts a maximum of 512 tokens
- The first token of a sequence is always `[CLS]`, and another special token is `[SEP]`
