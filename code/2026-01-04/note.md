Day 4 #AttentionIsAllYouNeed

### Transformer

- Encoder: 6 layers (Multi-Head Self-Attention -- Feed-Forward Network -- Residual Connection) 
- Decoder: 6 layers (Masked Multi-Head Self-Attention -- Encoder-Decoder Attention -- Feed-Forward Network -- Residual Connection + Layer Normalization)
