Day 4 #AttentionIsAllYouNeed
- Encoder: 6 layers (Multi-Head Self-Attention -- Feed-Forward Network -- Residual Connection) 
- Decoder: 6 layers (Masked Multi-Head Self-Attention -- Encoder-Decoder Attention -- Feed-Forward Network -- Residual Connection + Layer Normalization)
